{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormDense(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, classes=1000):\n",
    "        super(NormDense, self).__init__()\n",
    "        self.classes = classes\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(name='norm_dense_w', shape=(input_shape[-1], self.classes),\n",
    "                                 initializer='random_normal', trainable=True)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        norm_w = tf.nn.l2_normalize(self.w, axis=0)\n",
    "        x = tf.matmul(inputs, norm_w)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "n_classes = 16\n",
    "lr = 0.0001\n",
    "m1 = 1.0\n",
    "m2 = 0.2\n",
    "m3 = 0.3\n",
    "s = 64.\n",
    "img_w = 112\n",
    "img_h = 112\n",
    "\n",
    "data_dir = '/home/cocoza4/datasets/lfw_sample'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate =lr\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=2,\n",
    "    decay_rate=0.96,\n",
    "    staircase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArcFaceModel(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, backbone, n_classes):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.norm_dense = NormDense(n_classes)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        prelogits = self.backbone(inputs)\n",
    "        norm_dense = self.norm_dense(prelogits)\n",
    "        return prelogits, norm_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.training.Model at 0x7f4440716e80>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet50 = tf.keras.applications.ResNet50()\n",
    "resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ArcFaceModel(resnet50, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arcface_loss(x, normx_cos, labels, m1, m2, m3, s):\n",
    "    norm_x = tf.norm(x, axis=1, keepdims=True)\n",
    "    cos_theta = normx_cos / norm_x\n",
    "    theta = tf.acos(cos_theta)\n",
    "    mask = tf.one_hot(labels, depth=normx_cos.shape[-1])\n",
    "    zeros = tf.zeros_like(mask)\n",
    "    cond = tf.where(tf.greater(theta * m1 + m3, math.pi), zeros, mask)\n",
    "    cond = tf.cast(cond, dtype=tf.bool)\n",
    "    m1_theta_plus_m3 = tf.where(cond, theta * m1 + m3, theta)\n",
    "    cos_m1_theta_plus_m3 = tf.cos(m1_theta_plus_m3)\n",
    "    prelogits = tf.where(cond, cos_m1_theta_plus_m3 - m2, cos_m1_theta_plus_m3) * s\n",
    "\n",
    "    cce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)  # do softmax\n",
    "    loss = cce(labels, prelogits)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inputs, labels):\n",
    "    with tf.GradientTape(persistent=False) as tape:\n",
    "        prelogits, norm_dense = model(inputs, training=True)\n",
    "        arc_loss = arcface_loss(prelogits, norm_dense, labels, m1, m2, m3, s)\n",
    "        loss = arc_loss\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss\n",
    "\n",
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "    # training=False is only needed if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "    prelogits, norm_dense = model(images, training=False)\n",
    "    loss = arcface_loss(prelogits, norm_dense, labels, m1, m2, m3, s)\n",
    "\n",
    "    return loss\n",
    "\n",
    "@tf.function\n",
    "def predict_embedding(images):\n",
    "    prelogits, _ = model(images, training=False)\n",
    "    embeddings = tf.nn.l2_normalize(prelogits, axis=-1)\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(path, label):\n",
    "    raw = tf.io.read_file(path)\n",
    "    image = tf.image.decode_png(raw)\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = image / 255\n",
    "    image = tf.image.resize(image, (img_w, img_h))\n",
    "\n",
    "    # image = tf.image.resize(image, (224, 224))\n",
    "    # image = tf.image.random_crop(image, size=[112, 112, 3])\n",
    "    # image = tf.image.random_flip_left_right(image)\n",
    "\n",
    "    # image = image[None, ...]\n",
    "    return image, label\n",
    "\n",
    "def get_data(path):\n",
    "    ids = list(os.listdir(path))\n",
    "    ids.sort()\n",
    "    cat_num = len(ids)\n",
    "\n",
    "    id_dict = dict(zip(ids, list(range(cat_num))))\n",
    "    paths = []\n",
    "    labels = []\n",
    "    for i in ids:\n",
    "        cur_dir = os.path.join(path, i)\n",
    "        fns = os.listdir(cur_dir)\n",
    "        paths.append([os.path.join(cur_dir, fn) for fn in fns])\n",
    "        labels.append([id_dict[i]] * len(fns))\n",
    "\n",
    "    return paths, labels\n",
    "\n",
    "def generate(path, preprocess_fn=preprocess, batch_size=2):\n",
    "    paths, labels = get_data(path)\n",
    "    n_classes = len(paths)\n",
    "    paths = [path for cls in paths for path in cls]\n",
    "    labels = [label for cls in labels for label in cls]\n",
    "    \n",
    "    assert (len(paths) == len(labels))\n",
    "    \n",
    "    ds = (tf.data.Dataset.from_tensor_slices((paths, labels))\n",
    "          .cache()\n",
    "          .shuffle(20000)\n",
    "          .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "          .map(preprocess_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "          .batch(batch_size))\n",
    "    \n",
    "    return ds\n",
    "\n",
    "def predict_embeddings(model, sess, images, batch_size):\n",
    "    n_images = len(images)\n",
    "    batches = int(np.ceil(n_images / batch_size))\n",
    "    embs_array = np.zeros((n_images, model.emb_size))\n",
    "    \n",
    "    it = tqdm(range(batches), 'Predict embeddings')\n",
    "    for i in it:\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        images_batch = images[start:end]\n",
    "        images_batch = [read_image(file) for file in images_batch]\n",
    "        \n",
    "        embs = sess.run(model.embeddings, feed_dict={model.input_tensor: images_batch})\n",
    "        embs_array[start:end] = embs\n",
    "        \n",
    "    return embs_array\n",
    "\n",
    "def evaluate(model, summary, pairs, lfw_paths, actual_issame, batch_size, n_folds):\n",
    "    n_images = len(actual_issame) * 2\n",
    "    assert len(lfw_paths) == n_images\n",
    "\n",
    "    start_time = time.time()\n",
    "    embs_array = np.zeros((n_images, embedding_size))\n",
    "    it = tqdm(range(range(0, n_images, batch_size)), 'Predict embeddings')\n",
    "    for start in it:\n",
    "        end = start + bsize\n",
    "        embs_array[start:end] = predict_embeddings(lfw_paths[start:end]) \n",
    "        \n",
    "    _, _, accuracy, val, val_std, far = lfw.evaluate(embs_array, actual_issame, n_folds=n_folds)\n",
    "    \n",
    "    print('Accuracy: %1.3f+-%1.3f' % (np.mean(accuracy), np.std(accuracy)))\n",
    "    print('Validation rate: %2.5f+-%2.5f @ FAR=%2.5f' % (val, val_std, far))\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    return np.mean(accuracy), val\n",
    "#     # Add validation loss and accuracy to summary\n",
    "#     summary.scalar('test/loss', loss, step=global_step)\n",
    "#     summary.value.add(tag='lfw/accuracy', simple_value=np.mean(accuracy))\n",
    "#     summary.value.add(tag='lfw/val_rate', simple_value=val)\n",
    "#     summary.value.add(tag='lfw/time_elapsed', simple_value=elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_dir = '/tmp/logs'\n",
    "current_time = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "train_logdir = summary_dir # os.path.join(summary_dir, current_time, 'train')\n",
    "# valid_log_dir = os.path.join(summary_dir, current_time, 'valid')\n",
    "\n",
    "# summary_writer = tf.compat.v2.summary.create_file_writer(current_time)\n",
    "writer = tf.compat.v2.summary.create_file_writer(train_logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = 5\n",
    "epoch = 0\n",
    "eval_every = 1\n",
    "\n",
    "global_step = tf.Variable(0, name=\"global_step\", dtype=tf.int64)\n",
    "# global_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = generate(data_dir, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\tStep: 0\tTime 1.226\tLoss 35.090\n",
      "Epoch: 1\tStep: 1\tTime 1.196\tLoss 35.490\n",
      "Epoch: 1\tStep: 2\tTime 1.136\tLoss 34.749\n",
      "Epoch: 1\tStep: 3\tTime 1.137\tLoss 35.434\n",
      "Epoch: 1\tStep: 4\tTime 1.130\tLoss 36.000\n",
      "Epoch: 1\tStep: 5\tTime 1.131\tLoss 34.973\n",
      "Epoch: 1\tStep: 6\tTime 1.142\tLoss 35.068\n",
      "Epoch: 1\tStep: 7\tTime 1.134\tLoss 35.843\n",
      "Epoch: 1\tStep: 8\tTime 1.139\tLoss 35.903\n",
      "Epoch: 2\tStep: 9\tTime 1.141\tLoss 33.382\n",
      "Epoch: 2\tStep: 10\tTime 1.156\tLoss 33.426\n",
      "Epoch: 2\tStep: 11\tTime 1.118\tLoss 33.375\n",
      "Epoch: 2\tStep: 12\tTime 1.124\tLoss 32.984\n",
      "Epoch: 2\tStep: 13\tTime 1.174\tLoss 33.858\n",
      "Epoch: 2\tStep: 14\tTime 1.173\tLoss 33.395\n",
      "Epoch: 2\tStep: 15\tTime 1.174\tLoss 33.095\n",
      "Epoch: 2\tStep: 16\tTime 1.148\tLoss 32.548\n",
      "Epoch: 2\tStep: 17\tTime 1.624\tLoss 32.846\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    \n",
    "    summary = tf.Summary()\n",
    "    \n",
    "    \n",
    "    for inputs, targets in train_gen:\n",
    "        t1 = time.time()\n",
    "        loss = train_step(inputs, targets)\n",
    "        elapsed = time.time() - t1\n",
    "        with writer.as_default():\n",
    "            tf.compat.v2.summary.scalar('train/loss', loss, step=global_step)\n",
    "        \n",
    "        summary.value.add(tag='train/loss', simple_value=loss)\n",
    "        print('Epoch: %d\\tStep: %d\\tTime %.3f\\tLoss %2.3f' % \n",
    "                    (epoch+1, global_step, elapsed, loss))\n",
    "#         print('epoch: {}, step: {}, loss = {}'.format(epoch, step, loss))\n",
    "\n",
    "        global_step.assign_add(1)\n",
    "#         global_step += 1\n",
    "    \n",
    "        \n",
    "    if epoch % eval_every == 0 or epoch == epochs-1:\n",
    "        t1 = time.time()\n",
    "        evaluate(model, sess, summary, pairs, lfw_paths, actual_issame, args.batch_size, args.lfw_n_folds)\n",
    "        time_elapsed = time.time() - t1\n",
    "        tf.compat.v2.summary.scalar('lfw/time_elapsed', loss, step=global_step)\n",
    "    \n",
    "#     summary_writer.add_summary(summary, step)\n",
    "    writer.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img, label):\n",
    "    with tf.GradientTape(persistent=False) as tape:\n",
    "        prelogits, dense, norm_dense = self.model(img, training=True)\n",
    "\n",
    "        # sm_loss = softmax_loss(dense, label)\n",
    "        # norm_sm_loss = softmax_loss(norm_dense, label)\n",
    "        arc_loss = arcface_loss(prelogits, norm_dense, label, self.m1, self.m2, self.m3, self.s)\n",
    "        logit_loss = arc_loss\n",
    "\n",
    "        if self.centers is not None:\n",
    "            ct_loss, self.centers = center_loss(prelogits, label, self.centers, self.ct_alpha)\n",
    "        else:\n",
    "            ct_loss = 0\n",
    "\n",
    "        loss = logit_loss + self.ct_loss_factor * ct_loss\n",
    "    gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "    self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
    "\n",
    "    return loss, logit_loss, ct_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ar-models",
   "language": "python",
   "name": "ar-models"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
