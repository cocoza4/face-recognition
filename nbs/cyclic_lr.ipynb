{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('/home/cocoza4/workspace/absorouteio/asr-face-recognition/src')\n",
    "import math\n",
    "import time\n",
    "import argparse\n",
    "import logging\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import utils\n",
    "import losses\n",
    "import models\n",
    "from evaluate import lfw, cfp, eval_utils\n",
    "from generators import TFRecordDataGenerator\n",
    "from backbone.resnet import SEResNet \n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def predict_embedding(model, images):\n",
    "    x = model(images, training=False)\n",
    "    return tf.nn.l2_normalize(x, axis=-1)\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, inputs, labels, emb_weights, optimizer, loss_fn, global_batch_size):\n",
    "    with tf.GradientTape(persistent=False) as tape:\n",
    "        embeddings = model(inputs, training=True)\n",
    "        per_example_loss = loss_fn(embeddings, emb_weights, labels)\n",
    "        loss = tf.nn.compute_average_loss(per_example_loss, global_batch_size=global_batch_size)\n",
    "\n",
    "    trainable_vars = model.trainable_variables + [emb_weights]\n",
    "    gradients = tape.gradient(loss, trainable_vars)\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "    return loss\n",
    "\n",
    "def parse_example(proto):\n",
    "    feature_description = {\n",
    "        'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/label': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/height': tf.io.FixedLenFeature([], tf.int64)\n",
    "    }\n",
    "    tf_example = tf.io.parse_single_example(proto, feature_description)\n",
    "    return tf_example\n",
    "\n",
    "def _preprocess(image, training):\n",
    "    if training:\n",
    "        image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image -= 127.5\n",
    "    image *= 0.0078125\n",
    "    return image\n",
    "\n",
    "def preprocess_tf_example(example, training=True):\n",
    "    width = example['image/width']\n",
    "    height = example['image/height']\n",
    "    label = tf.cast(example['image/label'], tf.int32)\n",
    "    image = tf.io.decode_image(example['image/encoded'])\n",
    "    return _preprocess(image, training=training), label\n",
    "    \n",
    "def preprocess(path, training=True):\n",
    "    raw = tf.io.read_file(path)\n",
    "    image = tf.image.decode_image(raw)\n",
    "    _, _, c = image.shape\n",
    "    if c > 3 or c == 1:\n",
    "        image = utils.to_rgb(image.numpy())\n",
    "    return _preprocess(image, training=training)\n",
    "\n",
    "def load_ckpt(ckpt_dir, max_to_keep, backbone, global_step, model, optimizer, emb_weights):\n",
    "    ckpt = tf.train.Checkpoint(global_step=global_step, model=model, optimizer=optimizer, emb_weights=emb_weights)\n",
    "    ckpt_manager = tf.train.CheckpointManager(ckpt, ckpt_dir, max_to_keep=max_to_keep, checkpoint_name=backbone)\n",
    " \n",
    "    if ckpt_manager.latest_checkpoint:\n",
    "        ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "        logging.info(\"Restored from {}\".format(ckpt_manager.latest_checkpoint))\n",
    "    else:\n",
    "        logging.info(\"Initializing from scratch.\")\n",
    "\n",
    "    return ckpt_manager\n",
    "\n",
    "def evaluate(summary, global_step, eval_fn, predict_fn, tag, results_file):\n",
    "    t1 = time.time()\n",
    "    accuracy, val, far, frr = eval_fn(predict_fn=predict_fn)\n",
    "    time_elapsed = time.time() - t1\n",
    "    summary.scalar('%s/accuracy' % tag, accuracy, step=global_step)\n",
    "    summary.scalar('%s/val_rate' % tag, val, step=global_step)\n",
    "    summary.scalar('%s/far' % tag, far, step=global_step)\n",
    "    summary.scalar('%s/frr' % tag, frr, step=global_step)\n",
    "    summary.scalar('%s/time_elapsed' % tag, time_elapsed, step=global_step)\n",
    "    eval_utils.save_result(results_file, accuracy, val, far, frr)\n",
    "\n",
    "\n",
    "def get_optimizer(name, lr, **kwargs):\n",
    "    if name.lower() == 'adam':\n",
    "        opt = tf.keras.optimizers.Adam(lr)\n",
    "    elif name.lower() == 'sgd':\n",
    "        opt = tf.keras.optimizers.SGD(learning_rate=lr, momentum=kwargs['mom'], nesterov=True)\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 19\n",
    "m1 = 1.\n",
    "m2 = 0.5\n",
    "m3 = 0\n",
    "s = 64.\n",
    "mom = 0.9\n",
    "batch_size = 10\n",
    "embedding_size = 10\n",
    "\n",
    "train_dir = '/home/cocoza4/datasets/sample_asian_and_msra_mtcnnpy_112_margin32'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_fn(inputs):\n",
    "    preprocessed = np.array([preprocess(path, training=False).numpy() for path in inputs])\n",
    "    return predict_embedding(model, preprocessed)\n",
    "\n",
    "\n",
    "loss_fn = partial(losses.arcface_loss, n_classes=n_classes, m1=m1, m2=m2, m3=m3, s=s, \n",
    "                    reduction=tf.keras.losses.Reduction.NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0415 01:42:53.993669 139684702295872 cross_device_ops.py:1258] There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_template = 'Epoch: %d[%d/%d]\\tStep %d\\tTime %.3f\\tLoss %2.3f\\tlr %.5f'\n",
    "\n",
    "train_gen = TFRecordDataGenerator(train_dir, batch_size=batch_size)\n",
    "train_ds = train_gen.generate(example_parser=parse_example, preprocess_fn=preprocess_tf_example)\n",
    "    \n",
    "global_batch_size = 10 * strategy.num_replicas_in_sync\n",
    "train_iter = iter(train_ds)\n",
    "\n",
    "global_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "steps_per_epoch = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1[1/2]\tStep 0\tTime 6.832\tLoss 67.240\tlr 0.10000\n",
      "Epoch: 1[2/2]\tStep 1\tTime 2.485\tLoss 62.972\tlr 0.10000\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    global_step = tf.Variable(0, name=\"global_step\", dtype=tf.int64, trainable=False)\n",
    "    current_lr = 0.1\n",
    "    optimizer = get_optimizer('sgd', current_lr, mom=mom)\n",
    "\n",
    "    backbone = SEResNet(blocks=[1, 1, 1, 1])\n",
    "    model = models.ArcFaceModel(backbone, embedding_size)\n",
    "    \n",
    "    initializer = tf.initializers.VarianceScaling()\n",
    "    emb_weights = tf.Variable(initializer(shape=[n_classes, embedding_size]), \n",
    "                                name='embedding_weights', dtype=tf.float32)\n",
    "\n",
    "    for step in range(steps_per_epoch):\n",
    "        inputs, labels = next(train_iter)\n",
    "        t1 = time.time()\n",
    "        per_replica_loss = strategy.experimental_run_v2(train_step, args=(model, inputs, labels, emb_weights, \n",
    "                                                                            optimizer, loss_fn, global_batch_size,))\n",
    "        loss = strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n",
    "        elapsed = time.time() - t1\n",
    "\n",
    "        print(log_template % (epoch+1, step+1, steps_per_epoch, global_step.numpy(), elapsed, loss.numpy(), current_lr))\n",
    "\n",
    "        global_step.assign_add(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.lr.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=() dtype=float32, numpy=0.5>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.lr.assign(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CyclicLR:\n",
    "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
    "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn == None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma**(x)\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None, new_step_size=None):\n",
    "        \"\"\"Resets cycle iterations.\n",
    "        Optional boundary/step size adjustment.\n",
    "        \"\"\"\n",
    "        if new_base_lr != None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr != None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size != None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.\n",
    "        \n",
    "    def clr(self):\n",
    "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
    "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
    "    \n",
    "    @property\n",
    "    def lr(self):\n",
    "        if self.clr_iterations == 0:\n",
    "            return self.base_lr\n",
    "        else:\n",
    "            return self.clr()\n",
    "    \n",
    "    def step(self):\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "\n",
    "        self.history.setdefault('lr', []).append(self.lr)\n",
    "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.CyclicLR at 0x7f0a404e2a20>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clr = CyclicLR(base_lr=0.01, max_lr=0.1, mode='triangular')\n",
    "clr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clr.lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = []\n",
    "for _ in range(10000):\n",
    "    lrs.append(clr.lr)\n",
    "    clr.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.01,\n",
       " 0.010044999999999995,\n",
       " 0.01008999999999999,\n",
       " 0.010135000000000005,\n",
       " 0.01018,\n",
       " 0.010224999999999996,\n",
       " 0.010269999999999991,\n",
       " 0.010315000000000005,\n",
       " 0.010360000000000001,\n",
       " 0.010404999999999996,\n",
       " 0.01044999999999999,\n",
       " 0.010495000000000006,\n",
       " 0.01054,\n",
       " 0.010584999999999995,\n",
       " 0.01062999999999999,\n",
       " 0.010675000000000006,\n",
       " 0.01072,\n",
       " 0.010764999999999997,\n",
       " 0.010809999999999991,\n",
       " 0.010855000000000005,\n",
       " 0.010900000000000002,\n",
       " 0.010944999999999996,\n",
       " 0.010989999999999991,\n",
       " 0.011035000000000007,\n",
       " 0.011080000000000001,\n",
       " 0.011124999999999996,\n",
       " 0.01116999999999999,\n",
       " 0.011215000000000006,\n",
       " 0.011260000000000001,\n",
       " 0.011304999999999996,\n",
       " 0.011349999999999992,\n",
       " 0.011395000000000006,\n",
       " 0.011440000000000002,\n",
       " 0.011484999999999997,\n",
       " 0.011529999999999992,\n",
       " 0.011575000000000007,\n",
       " 0.011620000000000002,\n",
       " 0.011664999999999997,\n",
       " 0.011709999999999991,\n",
       " 0.011755000000000007,\n",
       " 0.011800000000000001,\n",
       " 0.011844999999999998,\n",
       " 0.011889999999999993,\n",
       " 0.011935000000000008,\n",
       " 0.011980000000000003,\n",
       " 0.012024999999999997,\n",
       " 0.012069999999999992,\n",
       " 0.012115000000000008,\n",
       " 0.012160000000000002,\n",
       " 0.012204999999999997,\n",
       " 0.012249999999999994,\n",
       " 0.012295000000000007,\n",
       " 0.012340000000000002,\n",
       " 0.012384999999999997,\n",
       " 0.012429999999999993,\n",
       " 0.012475000000000007,\n",
       " 0.012520000000000003,\n",
       " 0.012564999999999998,\n",
       " 0.012609999999999993,\n",
       " 0.012655000000000008,\n",
       " 0.012700000000000003,\n",
       " 0.012744999999999998,\n",
       " 0.012789999999999992,\n",
       " 0.012835000000000008,\n",
       " 0.012880000000000003,\n",
       " 0.012924999999999999,\n",
       " 0.012969999999999994,\n",
       " 0.01301500000000001,\n",
       " 0.013060000000000004,\n",
       " 0.013104999999999999,\n",
       " 0.013149999999999993,\n",
       " 0.013195000000000009,\n",
       " 0.013240000000000004,\n",
       " 0.013284999999999998,\n",
       " 0.013329999999999995,\n",
       " 0.013375000000000008,\n",
       " 0.013420000000000003,\n",
       " 0.013464999999999998,\n",
       " 0.013509999999999994,\n",
       " 0.013555000000000008,\n",
       " 0.013600000000000004,\n",
       " 0.013645,\n",
       " 0.013689999999999994,\n",
       " 0.01373500000000001,\n",
       " 0.013780000000000004,\n",
       " 0.013824999999999999,\n",
       " 0.013869999999999993,\n",
       " 0.013915000000000009,\n",
       " 0.013960000000000004,\n",
       " 0.014005,\n",
       " 0.014049999999999993,\n",
       " 0.01409500000000001,\n",
       " 0.014140000000000003,\n",
       " 0.014185,\n",
       " 0.014229999999999994,\n",
       " 0.01427500000000001,\n",
       " 0.014320000000000005,\n",
       " 0.014365,\n",
       " 0.014409999999999996,\n",
       " 0.01445500000000001,\n",
       " 0.014500000000000006,\n",
       " 0.014544999999999999,\n",
       " 0.014589999999999995,\n",
       " 0.014635000000000009,\n",
       " 0.014680000000000006,\n",
       " 0.014725,\n",
       " 0.014769999999999995,\n",
       " 0.01481500000000001,\n",
       " 0.014860000000000005,\n",
       " 0.014905,\n",
       " 0.014949999999999995,\n",
       " 0.01499500000000001,\n",
       " 0.015040000000000005,\n",
       " 0.015085000000000001,\n",
       " 0.015129999999999994,\n",
       " 0.015175000000000011,\n",
       " 0.015220000000000004,\n",
       " 0.015265,\n",
       " 0.015309999999999995,\n",
       " 0.01535499999999999,\n",
       " 0.015400000000000006,\n",
       " 0.015445,\n",
       " 0.015489999999999997,\n",
       " 0.01553500000000001,\n",
       " 0.015580000000000007,\n",
       " 0.015625,\n",
       " 0.015669999999999996,\n",
       " 0.015714999999999993,\n",
       " 0.015760000000000007,\n",
       " 0.015805,\n",
       " 0.015849999999999996,\n",
       " 0.015895000000000013,\n",
       " 0.015940000000000006,\n",
       " 0.015985,\n",
       " 0.016029999999999996,\n",
       " 0.016074999999999992,\n",
       " 0.016120000000000006,\n",
       " 0.016165000000000002,\n",
       " 0.016209999999999995,\n",
       " 0.016255000000000012,\n",
       " 0.016300000000000005,\n",
       " 0.016345000000000002,\n",
       " 0.016389999999999995,\n",
       " 0.01643499999999999,\n",
       " 0.01648000000000001,\n",
       " 0.016525,\n",
       " 0.016569999999999998,\n",
       " 0.01661499999999999,\n",
       " 0.016660000000000008,\n",
       " 0.016705,\n",
       " 0.016749999999999998,\n",
       " 0.01679499999999999,\n",
       " 0.016840000000000008,\n",
       " 0.016885000000000004,\n",
       " 0.016929999999999997,\n",
       " 0.01697499999999999,\n",
       " 0.017020000000000007,\n",
       " 0.017065000000000004,\n",
       " 0.017109999999999997,\n",
       " 0.017154999999999993,\n",
       " 0.017200000000000007,\n",
       " 0.017245000000000003,\n",
       " 0.017289999999999996,\n",
       " 0.017334999999999993,\n",
       " 0.017380000000000007,\n",
       " 0.017425000000000003,\n",
       " 0.01747,\n",
       " 0.017514999999999992,\n",
       " 0.017560000000000006,\n",
       " 0.017605000000000003,\n",
       " 0.01765,\n",
       " 0.017694999999999992,\n",
       " 0.01774000000000001,\n",
       " 0.017785000000000002,\n",
       " 0.01783,\n",
       " 0.017874999999999995,\n",
       " 0.01792000000000001,\n",
       " 0.017965000000000002,\n",
       " 0.018009999999999998,\n",
       " 0.018054999999999995,\n",
       " 0.01810000000000001,\n",
       " 0.018145,\n",
       " 0.018189999999999998,\n",
       " 0.018234999999999994,\n",
       " 0.018280000000000008,\n",
       " 0.018325,\n",
       " 0.018369999999999997,\n",
       " 0.018414999999999994,\n",
       " 0.01846000000000001,\n",
       " 0.018505000000000004,\n",
       " 0.018549999999999997,\n",
       " 0.018594999999999993,\n",
       " 0.01864000000000001,\n",
       " 0.018685000000000004,\n",
       " 0.018729999999999997,\n",
       " 0.018774999999999993,\n",
       " 0.01882000000000001,\n",
       " 0.018865000000000003,\n",
       " 0.01891,\n",
       " 0.018954999999999993,\n",
       " 0.01900000000000001,\n",
       " 0.019045000000000006,\n",
       " 0.01909,\n",
       " 0.019134999999999992,\n",
       " 0.01918000000000001,\n",
       " 0.019225000000000006,\n",
       " 0.01927,\n",
       " 0.019314999999999995,\n",
       " 0.01936000000000001,\n",
       " 0.019405000000000006,\n",
       " 0.019450000000000002,\n",
       " 0.019494999999999995,\n",
       " 0.01954000000000001,\n",
       " 0.019585000000000005,\n",
       " 0.01963,\n",
       " 0.019674999999999995,\n",
       " 0.01972000000000001,\n",
       " 0.019765000000000005,\n",
       " 0.01981,\n",
       " 0.019854999999999998,\n",
       " 0.019900000000000008,\n",
       " 0.019945000000000004,\n",
       " 0.01999,\n",
       " 0.020034999999999997,\n",
       " 0.02008000000000001,\n",
       " 0.020125000000000004,\n",
       " 0.02017,\n",
       " 0.020214999999999997,\n",
       " 0.02026000000000001,\n",
       " 0.020305000000000004,\n",
       " 0.02035,\n",
       " 0.020394999999999996,\n",
       " 0.02044000000000001,\n",
       " 0.020485000000000003,\n",
       " 0.02053,\n",
       " 0.020574999999999996,\n",
       " 0.020619999999999993,\n",
       " 0.020665000000000006,\n",
       " 0.02071,\n",
       " 0.020754999999999996,\n",
       " 0.020800000000000013,\n",
       " 0.020845000000000006,\n",
       " 0.02089,\n",
       " 0.020934999999999995,\n",
       " 0.02097999999999999,\n",
       " 0.021025000000000005,\n",
       " 0.021070000000000002,\n",
       " 0.021114999999999995,\n",
       " 0.021160000000000012,\n",
       " 0.02120500000000001,\n",
       " 0.02125,\n",
       " 0.021294999999999994,\n",
       " 0.02133999999999999,\n",
       " 0.021385000000000008,\n",
       " 0.02143,\n",
       " 0.021474999999999998,\n",
       " 0.02152000000000001,\n",
       " 0.021565000000000008,\n",
       " 0.021610000000000004,\n",
       " 0.021654999999999997,\n",
       " 0.02169999999999999,\n",
       " 0.021745000000000007,\n",
       " 0.021790000000000004,\n",
       " 0.021834999999999997,\n",
       " 0.02188000000000001,\n",
       " 0.021925000000000007,\n",
       " 0.021970000000000003,\n",
       " 0.022015,\n",
       " 0.022059999999999993,\n",
       " 0.022105000000000007,\n",
       " 0.022150000000000003,\n",
       " 0.022195,\n",
       " 0.022240000000000013,\n",
       " 0.022285000000000006,\n",
       " 0.022330000000000003,\n",
       " 0.022375,\n",
       " 0.022419999999999992,\n",
       " 0.022465000000000006,\n",
       " 0.022510000000000002,\n",
       " 0.022555,\n",
       " 0.022600000000000012,\n",
       " 0.022645000000000005,\n",
       " 0.022690000000000002,\n",
       " 0.022735,\n",
       " 0.022779999999999995,\n",
       " 0.02282500000000001,\n",
       " 0.02287,\n",
       " 0.022914999999999998,\n",
       " 0.022959999999999994,\n",
       " 0.023005000000000008,\n",
       " 0.02305,\n",
       " 0.023094999999999997,\n",
       " 0.023139999999999994,\n",
       " 0.023185000000000008,\n",
       " 0.023230000000000004,\n",
       " 0.023274999999999997,\n",
       " 0.023319999999999994,\n",
       " 0.02336500000000001,\n",
       " 0.023410000000000004,\n",
       " 0.023454999999999997,\n",
       " 0.023499999999999993,\n",
       " 0.02354500000000001,\n",
       " 0.023590000000000003,\n",
       " 0.023635,\n",
       " 0.023679999999999993,\n",
       " 0.02372500000000001,\n",
       " 0.023770000000000006,\n",
       " 0.023815,\n",
       " 0.023859999999999992,\n",
       " 0.02390500000000001,\n",
       " 0.023950000000000006,\n",
       " 0.023995,\n",
       " 0.024039999999999992,\n",
       " 0.02408500000000001,\n",
       " 0.024130000000000006,\n",
       " 0.024175000000000002,\n",
       " 0.024219999999999995,\n",
       " 0.02426500000000001,\n",
       " 0.024310000000000005,\n",
       " 0.024355,\n",
       " 0.024399999999999995,\n",
       " 0.02444500000000001,\n",
       " 0.024490000000000005,\n",
       " 0.024535,\n",
       " 0.024579999999999994,\n",
       " 0.024625000000000008,\n",
       " 0.024670000000000004,\n",
       " 0.024715,\n",
       " 0.024759999999999997,\n",
       " 0.024805000000000008,\n",
       " 0.024850000000000004,\n",
       " 0.024895,\n",
       " 0.024939999999999997,\n",
       " 0.02498500000000001,\n",
       " 0.025030000000000004,\n",
       " 0.025075,\n",
       " 0.025119999999999996,\n",
       " 0.02516500000000001,\n",
       " 0.025210000000000003,\n",
       " 0.025255,\n",
       " 0.025299999999999996,\n",
       " 0.02534500000000001,\n",
       " 0.025390000000000006,\n",
       " 0.025435,\n",
       " 0.025479999999999996,\n",
       " 0.025524999999999992,\n",
       " 0.025570000000000006,\n",
       " 0.025615,\n",
       " 0.025659999999999995,\n",
       " 0.025705000000000013,\n",
       " 0.02575000000000001,\n",
       " 0.025795,\n",
       " 0.025839999999999995,\n",
       " 0.02588499999999999,\n",
       " 0.02593000000000001,\n",
       " 0.025974999999999998,\n",
       " 0.026019999999999995,\n",
       " 0.026065000000000012,\n",
       " 0.026110000000000008,\n",
       " 0.026155000000000005,\n",
       " 0.026199999999999994,\n",
       " 0.02624499999999999,\n",
       " 0.026290000000000008,\n",
       " 0.026335000000000004,\n",
       " 0.026379999999999994,\n",
       " 0.02642500000000001,\n",
       " 0.026470000000000007,\n",
       " 0.026515000000000004,\n",
       " 0.02656,\n",
       " 0.02660499999999999,\n",
       " 0.026650000000000007,\n",
       " 0.026695000000000003,\n",
       " 0.02674,\n",
       " 0.02678500000000001,\n",
       " 0.026830000000000007,\n",
       " 0.026875000000000003,\n",
       " 0.02692,\n",
       " 0.02696499999999999,\n",
       " 0.027010000000000006,\n",
       " 0.027055000000000003,\n",
       " 0.0271,\n",
       " 0.02714500000000001,\n",
       " 0.027190000000000006,\n",
       " 0.027235000000000002,\n",
       " 0.02728,\n",
       " 0.027324999999999995,\n",
       " 0.027370000000000005,\n",
       " 0.027415000000000002,\n",
       " 0.02746,\n",
       " 0.027505000000000016,\n",
       " 0.027550000000000005,\n",
       " 0.027595,\n",
       " 0.027639999999999998,\n",
       " 0.027684999999999994,\n",
       " 0.027730000000000005,\n",
       " 0.027775,\n",
       " 0.027819999999999998,\n",
       " 0.027865000000000015,\n",
       " 0.02791000000000001,\n",
       " 0.027955,\n",
       " 0.027999999999999997,\n",
       " 0.028044999999999994,\n",
       " 0.02809000000000001,\n",
       " 0.028135,\n",
       " 0.028179999999999997,\n",
       " 0.028225000000000014,\n",
       " 0.02827000000000001,\n",
       " 0.028315000000000007,\n",
       " 0.028359999999999996,\n",
       " 0.028404999999999993,\n",
       " 0.02845000000000001,\n",
       " 0.028495000000000006,\n",
       " 0.028539999999999996,\n",
       " 0.028584999999999992,\n",
       " 0.02863000000000001,\n",
       " 0.028675000000000006,\n",
       " 0.028720000000000002,\n",
       " 0.028764999999999992,\n",
       " 0.02881000000000001,\n",
       " 0.028855000000000006,\n",
       " 0.028900000000000002,\n",
       " 0.02894499999999999,\n",
       " 0.02899000000000001,\n",
       " 0.029035000000000005,\n",
       " 0.02908,\n",
       " 0.02912499999999999,\n",
       " 0.02917000000000001,\n",
       " 0.029215000000000005,\n",
       " 0.02926,\n",
       " 0.029304999999999998,\n",
       " 0.029350000000000008,\n",
       " 0.029395000000000004,\n",
       " 0.02944,\n",
       " 0.029484999999999997,\n",
       " 0.029530000000000008,\n",
       " 0.029575000000000004,\n",
       " 0.02962,\n",
       " 0.029664999999999997,\n",
       " 0.029710000000000007,\n",
       " 0.029755000000000004,\n",
       " 0.0298,\n",
       " 0.029844999999999997,\n",
       " 0.029890000000000007,\n",
       " 0.029935000000000003,\n",
       " 0.02998,\n",
       " 0.030024999999999996,\n",
       " 0.030070000000000013,\n",
       " 0.030115000000000003,\n",
       " 0.03016,\n",
       " 0.030204999999999996,\n",
       " 0.030250000000000013,\n",
       " 0.030295000000000002,\n",
       " 0.03034,\n",
       " 0.030384999999999995,\n",
       " 0.030430000000000013,\n",
       " 0.03047500000000001,\n",
       " 0.03052,\n",
       " 0.030564999999999995,\n",
       " 0.030610000000000012,\n",
       " 0.03065500000000001,\n",
       " 0.030699999999999998,\n",
       " 0.030744999999999995,\n",
       " 0.030790000000000012,\n",
       " 0.030835000000000008,\n",
       " 0.030880000000000005,\n",
       " 0.030924999999999994,\n",
       " 0.03097000000000001,\n",
       " 0.031015000000000008,\n",
       " 0.031060000000000004,\n",
       " 0.031104999999999994,\n",
       " 0.03114999999999999,\n",
       " 0.031195000000000007,\n",
       " 0.031240000000000004,\n",
       " 0.03128499999999999,\n",
       " 0.03133000000000001,\n",
       " 0.03137500000000001,\n",
       " 0.031420000000000003,\n",
       " 0.031465,\n",
       " 0.03150999999999999,\n",
       " 0.03155500000000001,\n",
       " 0.0316,\n",
       " 0.031645,\n",
       " 0.03169000000000001,\n",
       " 0.031735000000000006,\n",
       " 0.03178,\n",
       " 0.031825,\n",
       " 0.031869999999999996,\n",
       " 0.031915000000000006,\n",
       " 0.03196,\n",
       " 0.032005,\n",
       " 0.03205000000000001,\n",
       " 0.032095000000000005,\n",
       " 0.03214,\n",
       " 0.032185,\n",
       " 0.032229999999999995,\n",
       " 0.032275000000000005,\n",
       " 0.03232,\n",
       " 0.032365,\n",
       " 0.032410000000000015,\n",
       " 0.032455000000000005,\n",
       " 0.0325,\n",
       " 0.032545,\n",
       " 0.032589999999999994,\n",
       " 0.03263500000000001,\n",
       " 0.03268,\n",
       " 0.032725,\n",
       " 0.032770000000000014,\n",
       " 0.03281500000000001,\n",
       " 0.03286,\n",
       " 0.032905,\n",
       " 0.03294999999999999,\n",
       " 0.03299500000000001,\n",
       " 0.03304,\n",
       " 0.033084999999999996,\n",
       " 0.033130000000000014,\n",
       " 0.03317500000000001,\n",
       " 0.033220000000000006,\n",
       " 0.033264999999999996,\n",
       " 0.03330999999999999,\n",
       " 0.03335500000000001,\n",
       " 0.033400000000000006,\n",
       " 0.033444999999999996,\n",
       " 0.03349000000000001,\n",
       " 0.03353500000000001,\n",
       " 0.033580000000000006,\n",
       " 0.033625,\n",
       " 0.03366999999999999,\n",
       " 0.03371500000000001,\n",
       " 0.033760000000000005,\n",
       " 0.033805,\n",
       " 0.03385000000000001,\n",
       " 0.03389500000000001,\n",
       " 0.033940000000000005,\n",
       " 0.033985,\n",
       " 0.03403,\n",
       " 0.03407500000000001,\n",
       " 0.034120000000000004,\n",
       " 0.034165,\n",
       " 0.03421000000000001,\n",
       " 0.03425500000000001,\n",
       " 0.034300000000000004,\n",
       " 0.034345,\n",
       " 0.03439,\n",
       " 0.03443500000000001,\n",
       " 0.034480000000000004,\n",
       " 0.034525,\n",
       " 0.03457000000000002,\n",
       " 0.03461500000000001,\n",
       " 0.03466,\n",
       " 0.034705,\n",
       " 0.034749999999999996,\n",
       " 0.03479500000000001,\n",
       " 0.03484,\n",
       " 0.034885,\n",
       " 0.03493000000000002,\n",
       " 0.03497500000000001,\n",
       " 0.03502,\n",
       " 0.035065,\n",
       " 0.035109999999999995,\n",
       " 0.03515500000000001,\n",
       " 0.0352,\n",
       " 0.035245,\n",
       " 0.035290000000000016,\n",
       " 0.03533499999999999,\n",
       " 0.03538000000000001,\n",
       " 0.035425,\n",
       " 0.035469999999999995,\n",
       " 0.03551500000000001,\n",
       " 0.03556000000000001,\n",
       " 0.035605,\n",
       " 0.035649999999999994,\n",
       " 0.03569499999999999,\n",
       " 0.03574000000000001,\n",
       " 0.035785000000000004,\n",
       " 0.035829999999999994,\n",
       " 0.03587500000000001,\n",
       " 0.03592000000000001,\n",
       " 0.035965000000000004,\n",
       " 0.03600999999999999,\n",
       " 0.03605499999999999,\n",
       " 0.03610000000000001,\n",
       " 0.036145000000000004,\n",
       " 0.03619,\n",
       " 0.03623500000000001,\n",
       " 0.03628000000000001,\n",
       " 0.036325,\n",
       " 0.03637,\n",
       " 0.03641499999999999,\n",
       " 0.036460000000000006,\n",
       " 0.036505,\n",
       " 0.03655,\n",
       " 0.03659500000000001,\n",
       " 0.036640000000000006,\n",
       " 0.036685,\n",
       " 0.03673,\n",
       " 0.036774999999999995,\n",
       " 0.036820000000000006,\n",
       " 0.036865,\n",
       " 0.03691,\n",
       " 0.036955000000000016,\n",
       " 0.037000000000000005,\n",
       " 0.037045,\n",
       " 0.03709,\n",
       " 0.037134999999999994,\n",
       " 0.037180000000000005,\n",
       " 0.037225,\n",
       " 0.03727,\n",
       " 0.037315000000000015,\n",
       " 0.037360000000000004,\n",
       " 0.037405,\n",
       " 0.03745,\n",
       " 0.037494999999999994,\n",
       " 0.03754000000000001,\n",
       " 0.037585,\n",
       " 0.03763,\n",
       " 0.037675000000000014,\n",
       " 0.03772000000000001,\n",
       " 0.037765,\n",
       " 0.037809999999999996,\n",
       " 0.03785499999999999,\n",
       " 0.03790000000000001,\n",
       " 0.037945000000000007,\n",
       " 0.037989999999999996,\n",
       " 0.03803500000000001,\n",
       " 0.03808000000000001,\n",
       " 0.038125000000000006,\n",
       " 0.038169999999999996,\n",
       " 0.03821499999999999,\n",
       " 0.03826000000000001,\n",
       " 0.038305000000000006,\n",
       " 0.03835,\n",
       " 0.03839500000000001,\n",
       " 0.03844000000000001,\n",
       " 0.038485000000000005,\n",
       " 0.03853,\n",
       " 0.03857499999999999,\n",
       " 0.03862000000000001,\n",
       " 0.038665000000000005,\n",
       " 0.03871,\n",
       " 0.03875500000000001,\n",
       " 0.03880000000000001,\n",
       " 0.038845000000000005,\n",
       " 0.03889,\n",
       " 0.038935,\n",
       " 0.03898000000000001,\n",
       " 0.039025000000000004,\n",
       " 0.03907,\n",
       " 0.03911500000000002,\n",
       " 0.03916000000000001,\n",
       " 0.039205000000000004,\n",
       " 0.03925,\n",
       " 0.039295,\n",
       " 0.03934000000000001,\n",
       " 0.039385,\n",
       " 0.03943,\n",
       " 0.03947500000000002,\n",
       " 0.039520000000000007,\n",
       " 0.039565,\n",
       " 0.03961,\n",
       " 0.039654999999999996,\n",
       " 0.03970000000000001,\n",
       " 0.039745,\n",
       " 0.03979,\n",
       " 0.039835000000000016,\n",
       " 0.03988000000000001,\n",
       " 0.039925,\n",
       " 0.03997,\n",
       " 0.040014999999999995,\n",
       " 0.04006000000000001,\n",
       " 0.04010500000000001,\n",
       " 0.04015,\n",
       " 0.040195000000000015,\n",
       " 0.04024000000000001,\n",
       " 0.04028500000000001,\n",
       " 0.04033,\n",
       " 0.040374999999999994,\n",
       " 0.04042000000000001,\n",
       " 0.04046500000000001,\n",
       " 0.040510000000000004,\n",
       " 0.040555000000000015,\n",
       " 0.04060000000000001,\n",
       " 0.04064500000000001,\n",
       " 0.040690000000000004,\n",
       " 0.04073499999999999,\n",
       " 0.04078000000000001,\n",
       " 0.04082500000000001,\n",
       " 0.040870000000000004,\n",
       " 0.040915000000000014,\n",
       " 0.04095999999999999,\n",
       " 0.04100500000000001,\n",
       " 0.04105,\n",
       " 0.041095,\n",
       " 0.04114000000000001,\n",
       " 0.041185000000000006,\n",
       " 0.04123,\n",
       " 0.041275,\n",
       " 0.041319999999999996,\n",
       " 0.041365000000000006,\n",
       " 0.04141,\n",
       " 0.041455,\n",
       " 0.041500000000000016,\n",
       " 0.041545000000000006,\n",
       " 0.04159,\n",
       " 0.041635,\n",
       " 0.041679999999999995,\n",
       " 0.04172500000000001,\n",
       " 0.04177,\n",
       " 0.041815,\n",
       " 0.041860000000000015,\n",
       " 0.04190500000000001,\n",
       " 0.04195,\n",
       " 0.041995,\n",
       " 0.042039999999999994,\n",
       " 0.04208500000000001,\n",
       " 0.04213000000000001,\n",
       " 0.042175,\n",
       " 0.042220000000000014,\n",
       " 0.04226500000000001,\n",
       " 0.04231000000000001,\n",
       " 0.042355,\n",
       " 0.04239999999999999,\n",
       " 0.04244500000000001,\n",
       " 0.04249000000000001,\n",
       " 0.042534999999999996,\n",
       " 0.042580000000000014,\n",
       " 0.04262500000000001,\n",
       " 0.04267000000000001,\n",
       " 0.042715,\n",
       " 0.04275999999999999,\n",
       " 0.04280500000000001,\n",
       " 0.042850000000000006,\n",
       " 0.042895,\n",
       " 0.04294000000000001,\n",
       " 0.04298500000000001,\n",
       " 0.043030000000000006,\n",
       " 0.043075,\n",
       " 0.04312,\n",
       " 0.04316500000000001,\n",
       " 0.043210000000000005,\n",
       " 0.043255,\n",
       " 0.04330000000000001,\n",
       " 0.04334500000000001,\n",
       " 0.043390000000000005,\n",
       " 0.043435,\n",
       " 0.04348,\n",
       " 0.04352500000000001,\n",
       " 0.043570000000000005,\n",
       " 0.043615,\n",
       " 0.04366000000000002,\n",
       " 0.04370500000000001,\n",
       " 0.043750000000000004,\n",
       " 0.043795,\n",
       " 0.04384,\n",
       " 0.043885000000000014,\n",
       " 0.043930000000000004,\n",
       " 0.043975,\n",
       " 0.04402000000000002,\n",
       " 0.044065000000000014,\n",
       " 0.04411,\n",
       " 0.044155,\n",
       " 0.044199999999999996,\n",
       " 0.044245000000000013,\n",
       " 0.04429,\n",
       " 0.044335,\n",
       " 0.04438000000000002,\n",
       " 0.04442500000000001,\n",
       " 0.04447000000000001,\n",
       " 0.044515,\n",
       " 0.044559999999999995,\n",
       " 0.04460500000000001,\n",
       " 0.04465000000000001,\n",
       " 0.044695,\n",
       " 0.044740000000000016,\n",
       " 0.04478500000000001,\n",
       " 0.04483000000000001,\n",
       " 0.044875000000000005,\n",
       " 0.044919999999999995,\n",
       " 0.04496500000000001,\n",
       " 0.04501000000000001,\n",
       " 0.045055000000000005,\n",
       " 0.045100000000000015,\n",
       " 0.04514500000000001,\n",
       " 0.04519000000000001,\n",
       " 0.045235000000000004,\n",
       " 0.04528,\n",
       " 0.04532500000000001,\n",
       " 0.04537000000000001,\n",
       " 0.045415000000000004,\n",
       " 0.045460000000000014,\n",
       " 0.04550500000000001,\n",
       " 0.04555000000000001,\n",
       " 0.045595000000000004,\n",
       " 0.04564,\n",
       " 0.04568500000000001,\n",
       " 0.04573000000000001,\n",
       " 0.045775,\n",
       " 0.04582000000000002,\n",
       " 0.04586500000000001,\n",
       " 0.045910000000000006,\n",
       " 0.045955,\n",
       " 0.046,\n",
       " 0.046045000000000016,\n",
       " 0.046090000000000006,\n",
       " 0.046135,\n",
       " 0.04618000000000002,\n",
       " 0.046225000000000016,\n",
       " 0.046270000000000006,\n",
       " 0.046315,\n",
       " 0.04636,\n",
       " 0.046405000000000016,\n",
       " 0.046450000000000005,\n",
       " 0.046495,\n",
       " 0.04654000000000002,\n",
       " 0.046584999999999994,\n",
       " 0.04663000000000001,\n",
       " 0.046675,\n",
       " 0.04672,\n",
       " 0.046765000000000015,\n",
       " 0.04681000000000001,\n",
       " 0.046855,\n",
       " 0.0469,\n",
       " 0.046944999999999994,\n",
       " 0.04699000000000001,\n",
       " 0.04703500000000001,\n",
       " 0.04708,\n",
       " 0.047125000000000014,\n",
       " 0.04717000000000001,\n",
       " 0.04721500000000001,\n",
       " 0.047259999999999996,\n",
       " 0.04730499999999999,\n",
       " 0.04735000000000001,\n",
       " 0.04739500000000001,\n",
       " 0.04744,\n",
       " 0.04748500000000001,\n",
       " 0.04753000000000001,\n",
       " 0.047575000000000006,\n",
       " 0.04762,\n",
       " 0.04766499999999999,\n",
       " 0.04771000000000001,\n",
       " 0.047755000000000006,\n",
       " 0.0478,\n",
       " 0.04784500000000001,\n",
       " 0.04789000000000001,\n",
       " 0.047935000000000005,\n",
       " 0.04798,\n",
       " 0.048025,\n",
       " 0.04807000000000001,\n",
       " 0.048115000000000005,\n",
       " 0.04816,\n",
       " 0.04820500000000002,\n",
       " 0.04825000000000001,\n",
       " 0.048295000000000005,\n",
       " 0.04834,\n",
       " 0.048385,\n",
       " 0.04843000000000001,\n",
       " 0.048475000000000004,\n",
       " 0.04852,\n",
       " 0.04856500000000002,\n",
       " 0.04861000000000001,\n",
       " 0.048655000000000004,\n",
       " 0.0487,\n",
       " 0.048745,\n",
       " 0.048790000000000014,\n",
       " 0.048835,\n",
       " 0.04888,\n",
       " 0.04892500000000002,\n",
       " 0.048970000000000014,\n",
       " 0.049015,\n",
       " 0.04906,\n",
       " 0.049104999999999996,\n",
       " 0.04915000000000001,\n",
       " 0.04919500000000001,\n",
       " 0.04924,\n",
       " 0.049285000000000016,\n",
       " 0.04933000000000001,\n",
       " 0.04937500000000001,\n",
       " 0.04942,\n",
       " 0.049464999999999995,\n",
       " 0.04951000000000001,\n",
       " 0.04955500000000001,\n",
       " 0.049600000000000005,\n",
       " 0.049645000000000016,\n",
       " 0.04969000000000001,\n",
       " 0.04973500000000001,\n",
       " 0.049780000000000005,\n",
       " 0.049824999999999994,\n",
       " 0.04987000000000001,\n",
       " 0.04991500000000001,\n",
       " 0.049960000000000004,\n",
       " 0.050005000000000015,\n",
       " 0.05005000000000001,\n",
       " 0.05009500000000001,\n",
       " 0.050140000000000004,\n",
       " 0.050185,\n",
       " 0.05023000000000001,\n",
       " 0.05027500000000001,\n",
       " 0.050320000000000004,\n",
       " 0.05036500000000002,\n",
       " 0.05041000000000001,\n",
       " 0.05045500000000001,\n",
       " 0.0505,\n",
       " 0.050545,\n",
       " 0.05059000000000001,\n",
       " 0.050635000000000006,\n",
       " 0.05068,\n",
       " 0.05072500000000002,\n",
       " 0.05077000000000001,\n",
       " 0.050815000000000006,\n",
       " 0.05086,\n",
       " 0.050905,\n",
       " 0.050950000000000016,\n",
       " 0.050995000000000006,\n",
       " 0.05104,\n",
       " 0.05108500000000002,\n",
       " 0.051130000000000016,\n",
       " 0.051175000000000005,\n",
       " 0.05122,\n",
       " 0.051265,\n",
       " 0.051310000000000015,\n",
       " 0.05135500000000001,\n",
       " 0.0514,\n",
       " 0.05144500000000002,\n",
       " 0.051490000000000015,\n",
       " 0.05153500000000001,\n",
       " 0.05158,\n",
       " 0.051625,\n",
       " 0.051670000000000015,\n",
       " 0.05171500000000001,\n",
       " 0.05176000000000001,\n",
       " 0.05180500000000002,\n",
       " 0.051850000000000014,\n",
       " 0.05189500000000001,\n",
       " 0.05194000000000001,\n",
       " 0.051984999999999996,\n",
       " 0.052030000000000014,\n",
       " 0.05207500000000001,\n",
       " 0.05212000000000001,\n",
       " 0.05216500000000002,\n",
       " 0.05220999999999999,\n",
       " 0.05225500000000001,\n",
       " 0.052300000000000006,\n",
       " 0.052345,\n",
       " 0.05239000000000001,\n",
       " 0.05243500000000001,\n",
       " 0.052480000000000006,\n",
       " 0.052525,\n",
       " 0.05256999999999999,\n",
       " 0.05261500000000001,\n",
       " 0.052660000000000005,\n",
       " 0.052705,\n",
       " 0.05275000000000001,\n",
       " 0.05279500000000001,\n",
       " 0.052840000000000005,\n",
       " 0.052885,\n",
       " 0.05293,\n",
       " 0.05297500000000001,\n",
       " 0.053020000000000005,\n",
       " 0.053065,\n",
       " 0.05311000000000002,\n",
       " 0.05315500000000001,\n",
       " 0.053200000000000004,\n",
       " 0.053245,\n",
       " 0.05329,\n",
       " 0.05333500000000001,\n",
       " 0.053380000000000004,\n",
       " 0.053425,\n",
       " 0.05347000000000002,\n",
       " 0.053515000000000014,\n",
       " 0.05356,\n",
       " 0.053605,\n",
       " 0.053649999999999996,\n",
       " 0.053695000000000014,\n",
       " 0.05374,\n",
       " 0.053785,\n",
       " 0.05383000000000002,\n",
       " 0.05387500000000001,\n",
       " 0.05392000000000001,\n",
       " 0.053965,\n",
       " 0.054009999999999996,\n",
       " 0.05405500000000001,\n",
       " 0.05410000000000001,\n",
       " 0.054145,\n",
       " 0.054190000000000016,\n",
       " 0.05423500000000001,\n",
       " 0.05428000000000001,\n",
       " 0.054325,\n",
       " 0.054369999999999995,\n",
       " 0.05441500000000001,\n",
       " 0.05446000000000001,\n",
       " 0.054505000000000005,\n",
       " 0.054550000000000015,\n",
       " 0.05459500000000001,\n",
       " 0.05464000000000001,\n",
       " 0.054685000000000004,\n",
       " 0.054729999999999994,\n",
       " 0.05477500000000001,\n",
       " 0.05482000000000001,\n",
       " 0.054865000000000004,\n",
       " 0.054910000000000014,\n",
       " 0.05495500000000001,\n",
       " ...]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CyclicLR(Callback):\n",
    "    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n",
    "    The method cycles the learning rate between two boundaries with\n",
    "    some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).\n",
    "    The amplitude of the cycle can be scaled on a per-iteration or \n",
    "    per-cycle basis.\n",
    "    This class has three built-in policies, as put forth in the paper.\n",
    "    \"triangular\":\n",
    "        A basic triangular cycle w/ no amplitude scaling.\n",
    "    \"triangular2\":\n",
    "        A basic triangular cycle that scales initial amplitude by half each cycle.\n",
    "    \"exp_range\":\n",
    "        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n",
    "        cycle iteration.\n",
    "    For more detail, please see paper.\n",
    "    \n",
    "    # Example\n",
    "        ```python\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., mode='triangular')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```\n",
    "    \n",
    "    Class also supports custom scaling functions:\n",
    "        ```python\n",
    "            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., scale_fn=clr_fn,\n",
    "                                scale_mode='cycle')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```    \n",
    "    # Arguments\n",
    "        base_lr: initial learning rate which is the\n",
    "            lower boundary in the cycle.\n",
    "        max_lr: upper boundary in the cycle. Functionally,\n",
    "            it defines the cycle amplitude (max_lr - base_lr).\n",
    "            The lr at any cycle is the sum of base_lr\n",
    "            and some scaling of the amplitude; therefore \n",
    "            max_lr may not actually be reached depending on\n",
    "            scaling function.\n",
    "        step_size: number of training iterations per\n",
    "            half cycle. Authors suggest setting step_size\n",
    "            2-8 x training iterations in epoch.\n",
    "        mode: one of {triangular, triangular2, exp_range}.\n",
    "            Default 'triangular'.\n",
    "            Values correspond to policies detailed above.\n",
    "            If scale_fn is not None, this argument is ignored.\n",
    "        gamma: constant in 'exp_range' scaling function:\n",
    "            gamma**(cycle iterations)\n",
    "        scale_fn: Custom scaling policy defined by a single\n",
    "            argument lambda function, where \n",
    "            0 <= scale_fn(x) <= 1 for all x >= 0.\n",
    "            mode paramater is ignored \n",
    "        scale_mode: {'cycle', 'iterations'}.\n",
    "            Defines whether scale_fn is evaluated on \n",
    "            cycle number or cycle iterations (training\n",
    "            iterations since start of cycle). Default is 'cycle'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
    "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
    "        super(CyclicLR, self).__init__()\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn == None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma**(x)\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
    "               new_step_size=None):\n",
    "        \"\"\"Resets cycle iterations.\n",
    "        Optional boundary/step size adjustment.\n",
    "        \"\"\"\n",
    "        if new_base_lr != None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr != None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size != None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.\n",
    "        \n",
    "    def clr(self):\n",
    "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
    "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        if self.clr_iterations == 0:\n",
    "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.lr, self.clr())        \n",
    "            \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        \n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "        \n",
    "        K.set_value(self.model.optimizer.lr, self.clr())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ar-models",
   "language": "python",
   "name": "ar-models"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
